\input{beamerpreamble.tex}
\begin{document}
\input{titlepage.tex}

\lecture{Classification tasks in NLP}{}

\section{Classification tasks in NLP}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Prepositional Phrases}
\begin{itemize}[<+->]
\item noun attach: {\em I bought the shirt with pockets}
\item verb attach: {\em I washed the shirt with soap}
\item As in the case of other attachment decisions in parsing: it depends on the meaning of the entire sentence -- needs world knowledge, etc.
\item Maybe there is a simpler solution: we can attempt to solve it using heuristics or associations between words
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Ambiguity Resolution: Prepositional Phrases in English}
  \begin{itemize}[<+->]
  \item Learning Prepositional Phrase Attachment: Annotated Data
\begin{tabular}{|cccc|c|}
\hline
$v$    &     $n_1$   &      $p$ & $n_2$ &       Attachment\\
\hline
join   &   board    &   as &  director & V \\
is       & chairman  &  of  & N.V.    & N \\
using  &   crocidolite & in  & filters & V \\
bring  &   attention  & to  & problem &  V \\ 
is      &  asbestos   & in &  products & N \\
making &   paper    &   for & filters & N \\
including & three     &  with & cancer &  N \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline
\end{tabular}
  \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Prepositional Phrase Attachment}
\begin{tabular}{|l|c|}  \hline
Method & Accuracy \\ \hline
Always noun attachment & 59.0 \\
Most likely for each preposition & 72.2 \\
Average Human (4 head words only) & 88.2 \\
Average Human (whole sentence) & 93.2 \\  \hline
\end{tabular}

\end{frame}

\begin{frame}
\frametitle{Back-off Smoothing}
\begin{itemize}[<+->]
\item Random variable $a$ represents attachment. 
\item $a = n_1$ or $a = v$ (two-class classification)
\item We want to compute probability of noun attachment: $p(a = n_1 \mid v, n_1, p, n_2)$. 
\item Probability of verb attachment is $1 - p(a = n_1 \mid v, n_1, p, n_2)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Back-off Smoothing}
\begin{enumerate}
\item<1-> If $f(v,n_1,p,n_2) > 0$ and $\hat{p} \neq 0.5$
\[ \hat{p}(1 \mid  v,n_1,p,n_2) = \frac{ f(1,v,n_1,p,n_2) }{ f(v,n_1,p,n_2)
} \]
\item<2-> Else if $f(v,n_1,p) + f(v,p,n_2) + f(n_1,p,n_2) > 0$ \\
and $\hat{p} \neq 0.5$
\[ \hat{p}(1 \mid  v,n_1,p,n_2) = \frac{ f(1,v,n_1,p) + f(1,v,p,n_2) +
  f(1,n_1,p,n_2) }{ f(v,n_1,p) + f(v,p,n_2) + f(n_1,p,n_2) } \]
\item<3-> Else if $f(v,p) + f(n_1,p) + f(p,n_2) > 0$
\[ \hat{p}(1 \mid  v,n_1,p,n_2) = \frac{ f(1,v,p) + f(1,n_1,p) +
  f(1,p,n_2) }{ f(v,p) + f(n_1,p) + f(p,n_2) } \]
\item<4-> Else if $f(p) > 0$ 
\[ \hat{p}(1 \mid  v,n_1,p,n_2) = \frac{ f(1,p) }{ f(p) } \]
\item<5-> Else \( \hat{p}(1 \mid  v,n_1,p,n_2) = 1.0 \)
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Prepositional Phrase Attachment: Results}
  \begin{itemize}[<+->]
  \item {\bf Results (Collins and Brooks 1995)}: 84.5\% accuracy \\
  with the use of some limited word classes for dates, numbers, etc.
  \item {\bf Toutanova, Manning, and Ng, 2004}: \\
use sophisticated smoothing model for PP attachment\\
86.18\% with words \& stems; with word classes: 87.54\%
  \item {\bf Merlo, Crocker and Berthouzoz, 1997}:\\
 test on multiple PPs, generalize disambiguation of 1 PP to 2-3 PPs\\
%14 structures possible for 3PPs assuming a single verb: all 14 are attested in the Treebank\\
%same model as CB95; but generalized to dealing with upto 3PPs\\
1PP: 84.3\% \ \ 2PP: 69.6\% \ \ 3PP: 43.6\% \\
{\bf Note that this is still not the real problem faced in parsing natural language}
  \end{itemize}
\end{frame}

\section{Probability models for Classification}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Probability Models}
\begin{itemize}[<+->]
\item $p(x,y)$: x = input, y = labels
\item Pick best prob distribution $p(x,y)$ to fit the data
\item Max likelihood of the data {\em according to the prob model} \\
equivalent to minimizing entropy
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Probability Models}
\begin{itemize}[<+->]
\item Max likelihood of the data {\em according to the prob model}
\item Equivalent to picking best parameter values $\theta$ such that
  the data gets highest likelihood:\\
\[ \textrm{max}_\theta p(\theta \mid \textrm{data} ) = \textrm{max}_\theta
  p(\theta) \cdot p(\textrm{data} \mid \theta) \]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log probabilities v.s. scores}
\begin{itemize}[<+->]
\item $n$-grams: $\ldots + \textrm{log}\ p(w_8 \mid w_6, w_7) + \ldots$
\item HMM: $\ldots + \textrm{log}\ p(t_5 \mid t_4) + \textrm{log}\ p(w_5 \mid t_5) + \ldots$
\item Naive Bayes: $\textrm{log}\ p(\textrm{class}) +
  \textrm{log}\ p(\textrm{feature}_1 \mid \textrm{class}) + \textrm{log}\ p(\textrm{feature}_2
  \mid \textrm{class}) + \ldots$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Advantages of probability models}
\begin{itemize}[<+->]
\item parameters can be estimated automatically, while scores have to
  twiddled by hand
\item parameters can be estimated from supervised or unsupervised data
\item probabilities can be used to quantify confidence in a particular
  state and used to compare against other probabilities in a strictly
  comparable setting
\item modularity: $p(semantics) \cdot p(syntax~\mid~semantics) \cdot$  \\
$p(morphology~\mid~syntax) \cdot p(phonology~\mid~morphology) \cdot p(sounds~\mid~phonology)$
\end{itemize}
\end{frame}

\lecture{Probabilistic Classifiers}{}

\section{Naive Bayes Classifier}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Naive Bayes Classifier}
\begin{itemize}[<+->]
\item \textbf{x} is the input that can be represented as $d$ independent features $f_j$, $1 \leq j \leq d$
\item $y$ is the output classification
\item $P(y \mid {\bf x}) = \frac{P(y) \cdot P({\bf x} \mid y)}{P({\bf x})}$ (Bayes Rule)
\item $P({\bf x} \mid y) = \prod_{j=1}^{d} P(f_j  \mid y)$
\item $P(y \mid {\bf x}) = P(y) \cdot  \prod_{j=1}^{d} P(f_j  \mid y)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using Naive Bayes for Document Classification}
\begin{itemize}[<+->]
\item Spam text: {\tt Learn how to make \$38.99 into a money making machine that
  pays ... \$7,000 / month !}
\item Distinguish spam text from regular email text
\item Find useful features to make this distinction
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using Naive Bayes}
\begin{itemize}[<+->]
\item Useful features
  \begin{enumerate}[<+->]
  \item contains {\tt turn \$AMOUNT into }
  \item contains {\tt \$AMOUNT}
  \item contains {\tt Learn how to }
  \item contains exclamation mark at end of sentence
  \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using Naive Bayes}
\begin{itemize}[<+->]
\item how many times do these features occur?
  \begin{enumerate}[<+->]
  \item contains: {\tt turn \$AMOUNT into }\\
    in spam text: $50$ \\
    in normal email: $2$\\
    i.e. 25x more likely in spam
  \item contains: {\tt \$AMOUNT} \\
    in spam text: $90$ \\
    in normal email: $10$\\
    i.e. 9x more likely in spam
  \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using Naive Bayes}
\begin{itemize}[<+->]
\item How likely is it for {\em both} features to occur at the same time
in a spam message?
  \begin{enumerate}[<+->]
  \item contains: {\tt turn \$AMOUNT into }
  \item contains: {\tt \$AMOUNT}
  \end{enumerate}
\item Assume we have a new feature, contains: {\tt turn \$AMOUNT into} {\em and} {\tt \$AMOUNT}
\item The model predicts that the event that both features occur
  simultaneously has probability $\frac{140}{152} = 0.92$
\item But Naive Bayes assumes that these features are independent
and should occur with probability: $0.92 \cdot 0.9 = 0.864$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using Naive Bayes}
\begin{itemize}[<+->]
\item Naive Bayes needs overlapping but independent features
\item How can we use all of the features we want?
  \begin{enumerate}[<+->]
  \item contains {\tt turn \$AMOUNT into }
  \item contains {\tt \$AMOUNT}
  \item contains {\tt Learn how to }
  \item contains exclamation mark at end of sentence
  \end{enumerate}
\item how about giving each feature a weight $w$ equal to its log probability: $w = \log p(f, y)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using Naive Bayes}
\begin{itemize}[<+->]
\item each feature gets a score equal to its log probability
\item Assign scores to features:
  \begin{enumerate}[<+->]
  \item $w_1 = +1$ contains {\tt turn \$AMOUNT into }
  \item $w_2 = +5$ contains {\tt \$AMOUNT}
  \item $w_3 = +1$ contains {\tt Learn how to }
  \item $w_4 = -2$ contains exclamation mark at end of sentence
  \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using Naive Bayes}
\begin{itemize}[<+->]
\item so add the scores and treat it like a log probability
\item $\textrm{log}\ p(\textit{spam} \mid \textit{feats}) = 4.2$
\item but then, $p(\textit{spam} \mid \textit{feats}) = exp(4.2) = 66.68$
\item how do we compute keep arbitrary scores and still get probabilities?
\end{itemize}
\end{frame}

\section{Log linear models}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item Let there be $m$ features, $f_k(\textbf{x}, y)$ for $k = 1, \ldots, m$
\item Define a parameter vector $\textbf{w} \in \mathbb{R}^m$
\item Each $(\textbf{x}, y)$ pair is mapped to score:
\[ s(\textbf{x}, y) = \sum_k w_k \cdot f_k(\textbf{x}, y) \]
\item Using inner product notation:
\begin{eqnarray*}
\textbf{w} \cdot \textbf{f}(\textbf{x}, y) & = & \sum_k w_k \cdot f_k(\textbf{x}, y) \\
s(\textbf{x}, y) & = & \textbf{w} \cdot \textbf{f}(\textbf{x}, y) 
\end{eqnarray*}
\item To get a probability from the score: Renormalize! 
\[ \Pr(y \mid \textbf{x}, \textbf{w}) = \frac{exp\left(s(\textbf{x}, y)\right)}{\sum_{y'} exp\left(s(\textbf{x}, y')\right)} \]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item The name `log-linear model' comes from:
\[ \log \Pr(y \mid \textbf{x}, \textbf{w}) = \underbrace{\textbf{w} \cdot \textbf{f}(\textbf{x}, y)}_{\textrm{linear term}} - \underbrace{\log \sum_{y'} exp\left( \textbf{w} \cdot \textbf{f}(\textbf{x}, y') \right)}_{\textrm{normalization term}} \]
\item Once the weights are learned, we can perform predictions using these features.
\item The goal: to find $\textbf{w}$ that maximizes the log likelihood $L(\textbf{w})$ of the labeled training set containing $(\textbf{x}_i, y_i)$ for $i = 1 \ldots n$
\begin{eqnarray*}
L(\textbf{w}) &=& \sum_{i} \log \Pr(y_i \mid \textbf{x}_i, \textbf{w}) \\
&=& \sum_i \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y_i) - \sum_i \log \sum_{y'} exp\left( \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y') \right) 
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item Maximize:
\begin{eqnarray*}
L(\textbf{w}) &=& \sum_i \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y_i) - \sum_i \log \sum_{y'} exp\left( \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y') \right) 
\end{eqnarray*}
\item Calculate gradient:
\begin{eqnarray*}
\lefteqn{\left. \frac{d L(\textbf{w})}{d \textbf{w}} \right|_{\textbf{w}} } \\
&=& \sum_i \textbf{f}(\textbf{x}_i, y_i) - \sum_i \frac{1}{\sum_{y''} exp\left(\textbf{w} \cdot \textbf{f}(x_i, y'')\right)} \\
&& \sum_{y'} \textbf{f}(\textbf{x}_i, y')  \cdot exp\left( \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y') \right) \\
&=& \sum_i \textbf{f}(\textbf{x}_i, y_i) - \sum_i \sum_{y'} \textbf{f}(\textbf{x}_i, y') \frac{exp\left( \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y') \right)}{\sum_{y''} exp\left(\textbf{w} \cdot \textbf{f}(x_i, y'')\right)} \\
&=& \underbrace{\sum_i \textbf{f}(\textbf{x}_i, y_i)}_{\textrm{Observed counts}} - \underbrace{\sum_i \sum_{y'} \textbf{f}(\textbf{x}_i, y') \Pr(y' \mid \textbf{x}_i, \textbf{w})}_{\textrm{Expected counts}}
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item Init: $\textbf{w}^{(0)} = \textbf{0}$
\item $t \leftarrow 0$
\item Iterate until convergence:
\begin{itemize}[<+->]
\item Calculate: $\Delta = \left. \frac{d L(\textbf{w})}{d \textbf{w}}  \right|_{\textbf{w} = \textbf{w}^{(t)}}$
\item Find $\beta^\ast = \arg\max_\beta L(\textbf{w}^{(t)} + \beta \Delta)$
\item Set $\textbf{w}^{(t+1)} \leftarrow \textbf{w}^{(t)} + \beta^\ast \Delta$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Learning the weights: $\textbf{w}$: Generalized Iterative
  Scaling}
\begin{tabbing}
$f^\# = max_{x,y} \sum_{j} f_j(x, y)$ \\
(the maximum possible feature value; needed for scaling) \pause \\
Initialize $\textbf{w}^{(0)}$ \pause \\
For \= each iteration $t$ \\
\> \texttt{expected[j]} $\leftarrow$ 0 for j = 1 .. \# of features \pause \\
\> For \= i = 1 to $\mid \textrm{training data} \mid$ \\
\>     \> For \= each feature $f_j$ \\
\>     \>     \> \texttt{expected[j]} += $f_j(x_i, y_i) \cdot P(y_i \mid x_i, \textbf{w}^{(t)})$ \pause \\
\> For \= each feature $f_j(x,y)$ \\
\>     \> \texttt{observed[j]} = $f_j(x, y) \cdot \frac{c(x,y)}{\mid \textrm{training data} \mid}$ \pause \\ 
\> For \= each feature $f_j(x,y)$ \\
\>     \> $w_j^{(t+1)} \leftarrow w_j^{(t)} \cdot \sqrt[f^\#]{\frac{\texttt{observed[j]}}{\texttt{expected[j]}}}$ 
\end{tabbing}
\par\noindent
\small{cf. Goodman, NIPS '01}
\end{frame} 

%\begin{frame}
%\frametitle{Maximum Entropy}
%\begin{itemize}[<+->]
%\item The log-linear model has an interpretation as a {\it maximum entropy} model.
%\item For observed events, maximize likelihood. For unobserved events, from all
%consistent models pick the one with maximum entropy.
%\item The maximum entropy principle: related to Occam's razor and
%  other similar justifications for scientific inquiry
%\item Make the minimum possible assumptions about unseen data
%\item Also: Laplace's {\em Principle of Insufficient Reason}: when one
%  has no information to distinguish between the probability of two
%  events, the best strategy is to consider them equally likely
%\end{itemize}
%\end{frame}

%\begin{frame}
%\frametitle{Logistic Regression}
%\begin{itemize}
%\item models effects of explanatory variables on binary valued
%  variable
%\item observations ${\bf x} = \{x_1, \ldots, x_j\}$ with success given
%  by $q({\bf x})$: \[ q({\bf x}) = \frac{e^{g({\bf x})}}{1 + e^{g({\bf
%  x})}} \] and \[ g({\bf x}) = \beta_0 + \sum_{j=1}^{k} \beta_j x_j \]
%\end{itemize} 
%\end{frame} 
%
%\begin{frame}
%\frametitle{Logistic Regression}
%\begin{itemize}
%\item probability that observations lead to success, or $p(a=1 \mid
%  b)$:
% \[ p(a=1 \mid b) = \frac{e^{g(b)}}{1 + e^{g(b)}} \] where
% \[ g(b) = \beta_0 f_0(1,b) + \sum_{j=1}^{k} \beta_j f_j(1,b) \]
%\item $\beta_j = \textrm{log} \alpha_j$, $f_0(1,b) = 1$ and $f_j(1,b) = x_j$
%\end{itemize} 
%\end{frame} 

\lecture{Linear models for Tagging}{}

\section{Tagging tasks in NLP}
\frame{\tableofcontents[currentsection]}
\newcommand{\postag}[1]{{\color{red}/#1}}
\newcommand{\nertag}[1]{{\color{blue}/#1}}

\begin{frame}
\frametitle{Tagging Tasks}
\begin{block}{Tagged Sequences}
a b e e a f h j $\Rightarrow$ a\postag{Y} b\postag{Z} e\postag{Y} e\postag{Y} a\postag{Z} f\postag{X} h\postag{Z} j\postag{Y}
\end{block}
\pause
\begin{alertblock}{Example 1: Part-of-speech tagging}
Profits\postag{N} soared\postag{V} at\postag{P} Boeing\postag{N} Co.\postag{N} ,\postag{,} easily\postag{ADV} topping\postag{V} forecasts\postag{N} on\postag{P} Wall\postag{N} Street\postag{N} ,\postag{,} as\postag{P} their\postag{POSS} CEO\postag{N} Alan\postag{N} Mulally\postag{N} announced\postag{V} first\postag{ADJ} quarter\postag{N} results\postag{N} .\postag{.}
\end{alertblock}
\pause
\begin{alertblock}{Example 2: Named Entity Recognition}
Profits\postag{O} soared\postag{O} at\postag{O} Boeing\nertag{B-CO} Co.\nertag{I-CO} ,\postag{O} easily\postag{O} topping\postag{O} forecasts\postag{O} on\postag{O} Wall\nertag{B-LOC} Street\nertag{I-LOC} ,\postag{O} as\postag{O} their\postag{O} CEO\postag{O} Alan\nertag{B-PER} Mulally\nertag{I-PER} announced\postag{O} first\postag{O} quarter\postag{O} results\postag{O} .\postag{O}
\end{alertblock}
\end{frame}


\begin{frame}
\frametitle{Notation for Tagging Tasks}
\begin{itemize}[<+->]
\item Set of possible input words: ${\cal V}$
\item Set of possible tags: ${\cal T}$
\item Tag sequence: $t_{[1:n]} = [t_1, \ldots, t_n]$
\item Training data is $N$ tagged sentences, the $i^{th}$ sentence has length $n_i$:
\[ (w_{[1:n]}^{(i)}, t_{[1:n]}^{(i)}) \textrm{ for } i = 1, \ldots, n \]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Independence Assumptions for Tagging}
\begin{block}{Chain Rule}
\[ P( t_{[1:n]} \mid w_{[1:n]}) = \pause \prod_{j=1}^n P(t_j \mid t_{j-1}, \ldots, t_1, w_{[1:n]}, j) \]
\end{block}
\pause
\begin{block}{Make independence assumptions}
\[ P( t_{[1:n]} \mid w_{[1:n]}) \approx \pause \prod_{j=1}^n P(t_j \mid t_{j-1}, w_{[1:n]}, j) \]
\centering {\small $j$ is the word being tagged}
\end{block}
\pause
\begin{block}{Questions}
\begin{itemize}
\item Split up $P(t_j \mid t_{j-1}, w_{[1:n]}, j)$ into parameters?
\item How to find $\arg\max_{t_{[1:n]}} P( t_{[1:n]} \mid w_{[1:n]})$?
\end{itemize}

\end{block}

\end{frame}

\input{acknowledgements.tex}

\end{document}
